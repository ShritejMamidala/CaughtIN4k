{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d51bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Config\n",
    "import os, csv, math, random, time, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "from pytorchvideo.models import x3d  # load X3D via PyTorchVideo\n",
    "\n",
    "from decord import VideoReader, cpu  # for video decoding\n",
    "\n",
    "import cv2  # for resizing frames\n",
    "\n",
    "# ---------- Config ----------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    csv_path: str = \"dataset_index.csv\"   # video_path,binary_label,type_label\n",
    "    clip_len: int = 16\n",
    "    stride: int = 8                       # overlap 50%\n",
    "    frame_size: int = 224\n",
    "    batch_size: int = 14               # per-clip\n",
    "    num_workers: int = 0\n",
    "    epochs: int = 10\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    # imbalance handling\n",
    "    pos_weight: Optional[float] = None    # if None, auto-compute from CSV\n",
    "    # 13 crime classes (0..12). Normal videos use type_label = -1\n",
    "    num_crime_classes: int = 13\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "\n",
    "cfg = Cfg()\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588ed0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset Parsing + Clip Loader\n",
    "\n",
    "def parse_csv(csv_path: str) -> List[Tuple[str, int, int]]:\n",
    "    items = []\n",
    "    with open(csv_path, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            vp = r[\"video_path\"]\n",
    "            b = int(r[\"binary_label\"])\n",
    "            t = int(r[\"type_label\"])\n",
    "            if os.path.exists(vp):\n",
    "                items.append((vp, b, t))\n",
    "    if not items:\n",
    "        raise RuntimeError(\"CSV parsed but no existing files found. Check paths in CSV.\")\n",
    "    return items\n",
    "\n",
    "def load_clip_decord(video_path: str, start_idx: int, clip_len: int, resize: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns tensor (3, T, H, W) normalized to [0,1] then ImageNet/Kinetics mean/std.\n",
    "    \"\"\"\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    fnum = len(vr)\n",
    "    inds = np.arange(start_idx, start_idx + clip_len)\n",
    "    inds = np.clip(inds, 0, fnum - 1)  # pad last frame if overflow\n",
    "    frames = vr.get_batch(inds)  # (T, H, W, 3), uint8\n",
    "    frames = frames.asnumpy()\n",
    "    # Resize each frame\n",
    "    frames_resized = np.stack([\n",
    "        cv2.resize(fr, (resize, resize), interpolation=cv2.INTER_AREA)\n",
    "        for fr in frames\n",
    "    ], axis=0)\n",
    "    frames_resized = frames_resized.astype(np.float32) / 255.0\n",
    "    # To torch (3, T, H, W)\n",
    "    frames_t = torch.from_numpy(frames_resized).permute(3, 0, 1, 2)\n",
    "    # Normalize using Kinetics / ImageNet style means and stds used in PyTorchVideo docs\n",
    "    mean = torch.tensor([0.45, 0.45, 0.45]).view(3,1,1,1)\n",
    "    std  = torch.tensor([0.225, 0.225, 0.225]).view(3,1,1,1)\n",
    "    frames_t = (frames_t - mean) / std\n",
    "    return frames_t\n",
    "\n",
    "class ClipDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazy version: generates clip start indices on the fly.\n",
    "    - Faster startup (no scanning all videos).\n",
    "    - Each __getitem__ samples a random clip from a video.\n",
    "    \"\"\"\n",
    "    def __init__(self, rows, clip_len=16, stride=8, frame_size=224, \n",
    "                 max_clips_per_video=None, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        rows: list of (video_path, binary_label, type_label)\n",
    "        mode: \"train\" = random clip per call, \"val\" = deterministic stride\n",
    "        \"\"\"\n",
    "        self.rows = rows\n",
    "        self.clip_len = clip_len\n",
    "        self.stride = stride\n",
    "        self.frame_size = frame_size\n",
    "        self.max_clips_per_video = max_clips_per_video\n",
    "        self.mode = mode\n",
    "\n",
    "        # preload fps + frame counts (cheap metadata)\n",
    "        self.video_meta = []\n",
    "        for i, (vp, b, t) in enumerate(self.rows):\n",
    "            try:\n",
    "                vr = VideoReader(vp, ctx=cpu(0))\n",
    "                fnum = len(vr)\n",
    "            except Exception:\n",
    "                fnum = 0\n",
    "            self.video_meta.append((fnum, vp, b, t))\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Scanned {i}/{len(self.rows)} videos\")   \n",
    "        print(f\"Scanned {i}/{len(self.rows)} videos\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # If max_clips_per_video is set, cap it\n",
    "        if self.max_clips_per_video:\n",
    "            return len(self.rows) * self.max_clips_per_video\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Pick video index\n",
    "        vidx = idx % len(self.rows)\n",
    "        fnum, vp, b, t = self.video_meta[vidx]\n",
    "\n",
    "        if fnum <= 0:\n",
    "            raise RuntimeError(f\"Video {vp} could not be read\")\n",
    "\n",
    "        # Training â†’ random start, Validation â†’ stride-based\n",
    "        if self.mode == \"train\":\n",
    "            max_start = max(1, fnum - self.clip_len)\n",
    "            start = np.random.randint(0, max_start + 1)\n",
    "        else:  # validation: pick evenly spaced windows\n",
    "            stride = max(1, fnum // max(1, self.max_clips_per_video or 8))\n",
    "            start = (idx // len(self.rows)) * stride\n",
    "            start = min(start, fnum - self.clip_len)\n",
    "\n",
    "        clip = load_clip_decord(vp, start, self.clip_len, self.frame_size)\n",
    "\n",
    "        return {\n",
    "            \"clip\": clip,\n",
    "            \"binary_label\": torch.tensor(float(b)),\n",
    "            \"type_label\": torch.tensor(t, dtype=torch.long),\n",
    "            \"video_idx\": torch.tensor(vidx, dtype=torch.long),\n",
    "            \"start_frame\": torch.tensor(start, dtype=torch.long),\n",
    "            \"video_path\": vp\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b1f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X3DMultiTask(nn.Module):\n",
    "    def __init__(self, backbone=None, num_classes=13):\n",
    "        super().__init__()\n",
    "        print(\"[ModelInit] Starting X3DMultiTask init\")\n",
    "\n",
    "        if backbone is None:\n",
    "            print(\"[ModelInit] Loading x3d_m backbone from torch.hub â€¦\")\n",
    "            backbone = torch.hub.load(\"facebookresearch/pytorchvideo\", \"x3d_m\", pretrained=True)\n",
    "            print(\"[ModelInit] Backbone loaded\")\n",
    "\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # --- Find classification head ---\n",
    "        if hasattr(self.backbone, \"head\") and hasattr(self.backbone.head, \"proj\"):\n",
    "            head = self.backbone.head\n",
    "            print(\"[ModelInit] Found head at self.backbone.head\")\n",
    "        elif hasattr(self.backbone, \"blocks\") and hasattr(self.backbone.blocks[-1], \"proj\"):\n",
    "            head = self.backbone.blocks[-1]\n",
    "            print(\"[ModelInit] Found head at self.backbone.blocks[-1]\")\n",
    "        else:\n",
    "            raise RuntimeError(\"[ModelInit] Could not locate X3D head\")\n",
    "\n",
    "        # --- Inspect feature dim ---\n",
    "        feat_dim = head.proj.in_features\n",
    "        print(f\"[ModelInit] Feature dimension detected: {feat_dim}\")\n",
    "\n",
    "        # --- Remove original classifier ---\n",
    "        head.proj = nn.Identity()\n",
    "        if hasattr(head, \"activation\"):\n",
    "            head.activation = nn.Identity()\n",
    "        print(\"[ModelInit] Replaced head.proj and head.activation with Identity()\")\n",
    "\n",
    "        # --- Define new heads ---\n",
    "        self.head_bin = nn.Linear(feat_dim, 1)\n",
    "        self.head_type = nn.Linear(feat_dim, num_classes)\n",
    "        print(\"[ModelInit] Added binary + type heads\")\n",
    "        print(\"[ModelInit] Init complete\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)     # (B, feat_dim)\n",
    "        s = self.head_bin(feats).squeeze(1)\n",
    "        c = self.head_type(feats)\n",
    "        return s, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7634ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Loss, Sampler & Training Helpers\n",
    "\n",
    "def compute_pos_weight(rows: List[Tuple[str,int,int]]):\n",
    "    pos = sum(1 for _, b, _ in rows if b == 1)\n",
    "    neg = sum(1 for _, b, _ in rows if b == 0)\n",
    "    if pos == 0:\n",
    "        return 1.0\n",
    "    return max(1.0, neg / max(1, pos))\n",
    "\n",
    "def make_sampler(rows: List[Tuple[str,int,int]]):\n",
    "    # rows[i] = (video_path, binary_label, type_label)\n",
    "    video_labels = [b for (_, b, _) in rows]\n",
    "    counts = np.bincount(video_labels, minlength=2)  # [neg, pos]\n",
    "    w_neg = 0.5 / (counts[0] + 1e-6)\n",
    "    w_pos = 0.5 / (counts[1] + 1e-6)\n",
    "    weights = [w_pos if y == 1 else w_neg for y in video_labels]\n",
    "    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optim, crit_bin, crit_type, device, log_interval=50):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_bin_acc = 0.0\n",
    "    total_type_acc = 0.0\n",
    "    type_count = 0\n",
    "\n",
    "    running_loss, running_correct_bin, running_total_bin = 0.0, 0, 0\n",
    "    running_correct_type, running_total_type = 0, 0\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        clips = batch[\"clip\"].to(device)\n",
    "        y_bin = batch[\"binary_label\"].to(device)\n",
    "        y_type = batch[\"type_label\"].to(device)\n",
    "\n",
    "        s, c = model(clips)\n",
    "        loss_b = crit_bin(s, y_bin)\n",
    "\n",
    "        mask = (y_type >= 0)\n",
    "        if mask.any():\n",
    "            loss_t = crit_type(c[mask], y_type[mask])\n",
    "            loss = loss_b + loss_t\n",
    "            type_acc = (c[mask].argmax(dim=1) == y_type[mask]).float().sum().item()\n",
    "            total_type_acc += type_acc\n",
    "            type_count += mask.sum().item()\n",
    "            running_correct_type += type_acc\n",
    "            running_total_type += mask.sum().item()\n",
    "        else:\n",
    "            loss = loss_b\n",
    "\n",
    "        # Binary accuracy\n",
    "        preds_bin = (torch.sigmoid(s) >= 0.5).float()\n",
    "        bin_acc = (preds_bin == y_bin).float().sum().item()\n",
    "        total_bin_acc += bin_acc\n",
    "        running_correct_bin += bin_acc\n",
    "        running_total_bin += clips.size(0)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        total_loss += loss.item() * clips.size(0)\n",
    "        running_loss += loss.item() * clips.size(0)\n",
    "\n",
    "        # ðŸ‘‡ Print every N batches\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            avg_bin = running_correct_bin / running_total_bin if running_total_bin > 0 else 0\n",
    "            avg_type = running_correct_type / running_total_type if running_total_type > 0 else 0\n",
    "            avg_loss = running_loss / running_total_bin if running_total_bin > 0 else 0\n",
    "            print(f\"[Train] Batch {i+1}/{len(loader)} \"\n",
    "                  f\"loss={avg_loss:.4f} bin_acc={avg_bin:.3f} type_acc={avg_type:.3f}\")\n",
    "            running_loss, running_correct_bin, running_total_bin = 0.0, 0, 0\n",
    "            running_correct_type, running_total_type = 0, 0\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_bin_acc = total_bin_acc / len(loader.dataset)\n",
    "    avg_type_acc = (total_type_acc / type_count) if type_count > 0 else 0.0\n",
    "    return {\"loss\": avg_loss, \"acc_bin\": avg_bin_acc, \"acc_type\": avg_type_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46deb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Validation & Main Training Loop\n",
    "def eval_epoch(model, loader, crit_bin, crit_type, device, log_interval=50):\n",
    "    print(\"[Eval] Starting validation loop â€¦\")\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_bin_acc = 0.0\n",
    "    total_type_acc = 0.0\n",
    "    type_count = 0\n",
    "\n",
    "    running_loss, running_correct_bin, running_total_bin = 0.0, 0, 0\n",
    "    running_correct_type, running_total_type = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            clips = batch[\"clip\"].to(device)\n",
    "            y_bin = batch[\"binary_label\"].to(device)\n",
    "            y_type = batch[\"type_label\"].to(device)\n",
    "\n",
    "            s, c = model(clips)\n",
    "            loss_b = crit_bin(s, y_bin)\n",
    "\n",
    "            mask = (y_type >= 0)\n",
    "            if mask.any():\n",
    "                loss_t = crit_type(c[mask], y_type[mask])\n",
    "                loss = loss_b + loss_t\n",
    "                type_acc = (c[mask].argmax(dim=1) == y_type[mask]).float().sum().item()\n",
    "                total_type_acc += type_acc\n",
    "                type_count += mask.sum().item()\n",
    "                running_correct_type += type_acc\n",
    "                running_total_type += mask.sum().item()\n",
    "            else:\n",
    "                loss = loss_b\n",
    "\n",
    "            preds_bin = (torch.sigmoid(s) >= 0.5).float()\n",
    "            bin_acc = (preds_bin == y_bin).float().sum().item()\n",
    "            total_bin_acc += bin_acc\n",
    "            running_correct_bin += bin_acc\n",
    "            running_total_bin += clips.size(0)\n",
    "\n",
    "            total_loss += loss.item() * clips.size(0)\n",
    "            running_loss += loss.item() * clips.size(0)\n",
    "\n",
    "            # ðŸ‘‡ log every N batches\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                avg_bin = running_correct_bin / running_total_bin if running_total_bin > 0 else 0\n",
    "                avg_type = running_correct_type / running_total_type if running_total_type > 0 else 0\n",
    "                avg_loss = running_loss / running_total_bin if running_total_bin > 0 else 0\n",
    "                print(f\"[Eval] Batch {i+1}/{len(loader)} \"\n",
    "                      f\"loss={avg_loss:.4f} bin_acc={avg_bin:.3f} type_acc={avg_type:.3f}\")\n",
    "                running_loss, running_correct_bin, running_total_bin = 0.0, 0, 0\n",
    "                running_correct_type, running_total_type = 0, 0\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_bin_acc = total_bin_acc / len(loader.dataset)\n",
    "    avg_type_acc = (total_type_acc / type_count) if type_count > 0 else 0.0\n",
    "    return {\"loss\": avg_loss, \"acc_bin\": avg_bin_acc, \"acc_type\": avg_type_acc}\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed(cfg.seed)\n",
    "    rows = parse_csv(cfg.csv_path)\n",
    "    random.shuffle(rows)\n",
    "    split = int(0.9 * len(rows))\n",
    "    rows_tr, rows_va = rows[:split], rows[split:]\n",
    "\n",
    "    # ðŸ”¹ filter validation to only crime\n",
    "    rows_va = [r for r in rows_va if r[1] == 1]\n",
    "    print(f\"[INFO] #Train videos: {len(rows_tr)}, #Val crime videos: {len(rows_va)}\")\n",
    "\n",
    "    cfg_pos = cfg.pos_weight if cfg.pos_weight is not None else compute_pos_weight(rows_tr)\n",
    "    print(f\"[INFO] Pos_weight={cfg_pos:.2f}\")\n",
    "\n",
    "    ds_tr = ClipDataset(rows_tr, clip_len=cfg.clip_len, stride=cfg.stride,\n",
    "                        frame_size=cfg.frame_size, mode=\"train\")\n",
    "\n",
    "    # smaller validation set (crime-only)\n",
    "    rows_va_small = random.sample(rows_va, k=min(50, len(rows_va)))  # still crime-only\n",
    "    ds_va = ClipDataset(rows_va_small, clip_len=cfg.clip_len, stride=cfg.stride,\n",
    "                        frame_size=cfg.frame_size, max_clips_per_video=4, mode=\"val\")\n",
    "\n",
    "    # Build sampler for training\n",
    "    sampler = make_sampler(rows_tr)\n",
    "\n",
    "    loader_tr = DataLoader(\n",
    "        ds_tr,\n",
    "        batch_size=cfg.batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    loader_va = DataLoader(\n",
    "        ds_va,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = X3DMultiTask(num_classes=cfg.num_crime_classes).to(cfg.device)\n",
    "    crit_bin = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([cfg_pos], device=cfg.device))\n",
    "    crit_type = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        t0 = time.time()\n",
    "        tr = train_epoch(model, loader_tr, opt, crit_bin, crit_type, cfg.device)\n",
    "        va = eval_epoch(model, loader_va, crit_bin, crit_type, cfg.device)\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        print(f\"[{epoch:02d}] \"\n",
    "              f\"train loss {tr['loss']:.4f} | bin acc {tr['acc_bin']:.3f} | type acc {tr['acc_type']:.3f} || \"\n",
    "              f\"val loss {va['loss']:.4f} | bin acc {va['acc_bin']:.3f} | type acc {va['acc_type']:.3f} | {dt:.1f}s\")\n",
    "\n",
    "        # ðŸ”¹ save every epoch\n",
    "        torch.save(model.state_dict(), f\"x3d_multitask_epoch{epoch:02d}.pt\")\n",
    "        print(f\"  â†³ saved x3d_multitask_epoch{epoch:02d}.pt\")\n",
    "\n",
    "        # ðŸ”¹ also track \"best\" model\n",
    "        if va[\"loss\"] < best_val:\n",
    "            best_val = va[\"loss\"]\n",
    "            torch.save(model.state_dict(), \"x3d_multitask_best.pt\")\n",
    "            print(\"  â†³ updated best model: x3d_multitask_best.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c3da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] #Train videos: 12843, #Val crime videos: 88\n",
      "[INFO] Pos_weight=13.90\n",
      "Scanned 0/12843 videos\n",
      "Scanned 100/12843 videos\n",
      "Scanned 200/12843 videos\n",
      "Scanned 300/12843 videos\n",
      "Scanned 400/12843 videos\n",
      "Scanned 500/12843 videos\n",
      "Scanned 600/12843 videos\n",
      "Scanned 700/12843 videos\n",
      "Scanned 800/12843 videos\n",
      "Scanned 900/12843 videos\n",
      "Scanned 1000/12843 videos\n",
      "Scanned 1100/12843 videos\n",
      "Scanned 1200/12843 videos\n",
      "Scanned 1300/12843 videos\n",
      "Scanned 1400/12843 videos\n",
      "Scanned 1500/12843 videos\n",
      "Scanned 1600/12843 videos\n",
      "Scanned 1700/12843 videos\n",
      "Scanned 1800/12843 videos\n",
      "Scanned 1900/12843 videos\n",
      "Scanned 2000/12843 videos\n",
      "Scanned 2100/12843 videos\n",
      "Scanned 2200/12843 videos\n",
      "Scanned 2300/12843 videos\n",
      "Scanned 2400/12843 videos\n",
      "Scanned 2500/12843 videos\n",
      "Scanned 2600/12843 videos\n",
      "Scanned 2700/12843 videos\n",
      "Scanned 2800/12843 videos\n",
      "Scanned 2900/12843 videos\n",
      "Scanned 3000/12843 videos\n",
      "Scanned 3100/12843 videos\n",
      "Scanned 3200/12843 videos\n",
      "Scanned 3300/12843 videos\n",
      "Scanned 3400/12843 videos\n",
      "Scanned 3500/12843 videos\n",
      "Scanned 3600/12843 videos\n",
      "Scanned 3700/12843 videos\n",
      "Scanned 3800/12843 videos\n",
      "Scanned 3900/12843 videos\n",
      "Scanned 4000/12843 videos\n",
      "Scanned 4100/12843 videos\n",
      "Scanned 4200/12843 videos\n",
      "Scanned 4300/12843 videos\n",
      "Scanned 4400/12843 videos\n",
      "Scanned 4500/12843 videos\n",
      "Scanned 4600/12843 videos\n",
      "Scanned 4700/12843 videos\n",
      "Scanned 4800/12843 videos\n",
      "Scanned 4900/12843 videos\n",
      "Scanned 5000/12843 videos\n",
      "Scanned 5100/12843 videos\n",
      "Scanned 5200/12843 videos\n",
      "Scanned 5300/12843 videos\n",
      "Scanned 5400/12843 videos\n",
      "Scanned 5500/12843 videos\n",
      "Scanned 5600/12843 videos\n",
      "Scanned 5700/12843 videos\n",
      "Scanned 5800/12843 videos\n",
      "Scanned 5900/12843 videos\n",
      "Scanned 6000/12843 videos\n",
      "Scanned 6100/12843 videos\n",
      "Scanned 6200/12843 videos\n",
      "Scanned 6300/12843 videos\n",
      "Scanned 6400/12843 videos\n",
      "Scanned 6500/12843 videos\n",
      "Scanned 6600/12843 videos\n",
      "Scanned 6700/12843 videos\n",
      "Scanned 6800/12843 videos\n",
      "Scanned 6900/12843 videos\n",
      "Scanned 7000/12843 videos\n",
      "Scanned 7100/12843 videos\n",
      "Scanned 7200/12843 videos\n",
      "Scanned 7300/12843 videos\n",
      "Scanned 7400/12843 videos\n",
      "Scanned 7500/12843 videos\n",
      "Scanned 7600/12843 videos\n",
      "Scanned 7700/12843 videos\n",
      "Scanned 7800/12843 videos\n",
      "Scanned 7900/12843 videos\n",
      "Scanned 8000/12843 videos\n",
      "Scanned 8100/12843 videos\n",
      "Scanned 8200/12843 videos\n",
      "Scanned 8300/12843 videos\n",
      "Scanned 8400/12843 videos\n",
      "Scanned 8500/12843 videos\n",
      "Scanned 8600/12843 videos\n",
      "Scanned 8700/12843 videos\n",
      "Scanned 8800/12843 videos\n",
      "Scanned 8900/12843 videos\n",
      "Scanned 9000/12843 videos\n",
      "Scanned 9100/12843 videos\n",
      "Scanned 9200/12843 videos\n",
      "Scanned 9300/12843 videos\n",
      "Scanned 9400/12843 videos\n",
      "Scanned 9500/12843 videos\n",
      "Scanned 9600/12843 videos\n",
      "Scanned 9700/12843 videos\n",
      "Scanned 9800/12843 videos\n",
      "Scanned 9900/12843 videos\n",
      "Scanned 10000/12843 videos\n",
      "Scanned 10100/12843 videos\n",
      "Scanned 10200/12843 videos\n",
      "Scanned 10300/12843 videos\n",
      "Scanned 10400/12843 videos\n",
      "Scanned 10500/12843 videos\n",
      "Scanned 10600/12843 videos\n",
      "Scanned 10700/12843 videos\n",
      "Scanned 10800/12843 videos\n",
      "Scanned 10900/12843 videos\n",
      "Scanned 11000/12843 videos\n",
      "Scanned 11100/12843 videos\n",
      "Scanned 11200/12843 videos\n",
      "Scanned 11300/12843 videos\n",
      "Scanned 11400/12843 videos\n",
      "Scanned 11500/12843 videos\n",
      "Scanned 11600/12843 videos\n",
      "Scanned 11700/12843 videos\n",
      "Scanned 11800/12843 videos\n",
      "Scanned 11900/12843 videos\n",
      "Scanned 12000/12843 videos\n",
      "Scanned 12100/12843 videos\n",
      "Scanned 12200/12843 videos\n",
      "Scanned 12300/12843 videos\n",
      "Scanned 12400/12843 videos\n",
      "Scanned 12500/12843 videos\n",
      "Scanned 12600/12843 videos\n",
      "Scanned 12700/12843 videos\n",
      "Scanned 12800/12843 videos\n",
      "Scanned 12842/12843 videos\n",
      "Scanned 0/50 videos\n",
      "Scanned 49/50 videos\n",
      "[ModelInit] Starting X3DMultiTask init\n",
      "[ModelInit] Loading x3d_m backbone from torch.hub â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\shrit/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInit] Backbone loaded\n",
      "[ModelInit] Found head at self.backbone.blocks[-1]\n",
      "[ModelInit] Feature dimension detected: 2048\n",
      "[ModelInit] Replaced head.proj and head.activation with Identity()\n",
      "[ModelInit] Added binary + type heads\n",
      "[ModelInit] Init complete\n",
      "[Train] Batch 50/917 loss=4.2376 bin_acc=0.503 type_acc=0.111\n",
      "[Train] Batch 100/917 loss=3.1087 bin_acc=0.661 type_acc=0.244\n",
      "[Train] Batch 150/917 loss=2.7350 bin_acc=0.881 type_acc=0.247\n",
      "[Train] Batch 200/917 loss=2.5367 bin_acc=0.917 type_acc=0.335\n",
      "[Train] Batch 250/917 loss=2.3978 bin_acc=0.959 type_acc=0.271\n",
      "[Train] Batch 300/917 loss=2.3702 bin_acc=0.924 type_acc=0.351\n",
      "[Train] Batch 350/917 loss=2.1665 bin_acc=0.959 type_acc=0.375\n",
      "[Train] Batch 400/917 loss=1.9742 bin_acc=0.956 type_acc=0.419\n",
      "[Train] Batch 450/917 loss=1.9731 bin_acc=0.971 type_acc=0.460\n",
      "[Train] Batch 500/917 loss=1.9192 bin_acc=0.951 type_acc=0.443\n",
      "[Train] Batch 550/917 loss=1.8111 bin_acc=0.961 type_acc=0.515\n",
      "[Train] Batch 600/917 loss=1.7696 bin_acc=0.973 type_acc=0.486\n",
      "[Train] Batch 650/917 loss=1.6741 bin_acc=0.967 type_acc=0.543\n",
      "[Train] Batch 700/917 loss=1.5986 bin_acc=0.964 type_acc=0.522\n",
      "[Train] Batch 750/917 loss=1.5250 bin_acc=0.983 type_acc=0.562\n",
      "[Train] Batch 800/917 loss=1.4045 bin_acc=0.980 type_acc=0.583\n",
      "[Train] Batch 850/917 loss=1.4236 bin_acc=0.974 type_acc=0.573\n",
      "[Train] Batch 900/917 loss=1.3209 bin_acc=0.983 type_acc=0.630\n",
      "[Eval] Starting validation loop â€¦\n",
      "[01] train loss 2.0968 | bin acc 0.915 | type acc 0.429 || val loss 2.2332 | bin acc 0.995 | type acc 0.365 | 1135.3s\n",
      "  â†³ saved x3d_multitask_epoch01.pt\n",
      "  â†³ updated best model: x3d_multitask_best.pt\n",
      "[Train] Batch 50/917 loss=1.3007 bin_acc=0.977 type_acc=0.661\n",
      "[Train] Batch 100/917 loss=1.1772 bin_acc=0.961 type_acc=0.685\n",
      "[Train] Batch 150/917 loss=1.4160 bin_acc=0.973 type_acc=0.617\n",
      "[Train] Batch 200/917 loss=1.2077 bin_acc=0.966 type_acc=0.684\n",
      "[Train] Batch 250/917 loss=1.0585 bin_acc=0.977 type_acc=0.733\n",
      "[Train] Batch 300/917 loss=0.9862 bin_acc=0.971 type_acc=0.739\n",
      "[Train] Batch 350/917 loss=0.9528 bin_acc=0.970 type_acc=0.766\n",
      "[Train] Batch 400/917 loss=0.9256 bin_acc=0.977 type_acc=0.744\n",
      "[Train] Batch 450/917 loss=0.7455 bin_acc=0.981 type_acc=0.806\n",
      "[Train] Batch 500/917 loss=0.9636 bin_acc=0.980 type_acc=0.756\n",
      "[Train] Batch 550/917 loss=0.8558 bin_acc=0.979 type_acc=0.774\n",
      "[Train] Batch 600/917 loss=0.7428 bin_acc=0.983 type_acc=0.813\n",
      "[Train] Batch 650/917 loss=0.6426 bin_acc=0.980 type_acc=0.855\n",
      "[Train] Batch 700/917 loss=0.6810 bin_acc=0.987 type_acc=0.811\n",
      "[Train] Batch 750/917 loss=0.7754 bin_acc=0.979 type_acc=0.788\n",
      "[Train] Batch 800/917 loss=0.7288 bin_acc=0.990 type_acc=0.805\n",
      "[Train] Batch 850/917 loss=0.5346 bin_acc=0.993 type_acc=0.881\n",
      "[Train] Batch 900/917 loss=0.6128 bin_acc=0.979 type_acc=0.866\n",
      "[Eval] Starting validation loop â€¦\n",
      "[02] train loss 0.8966 | bin acc 0.978 | type acc 0.768 || val loss 3.2398 | bin acc 0.960 | type acc 0.365 | 1126.6s\n",
      "  â†³ saved x3d_multitask_epoch02.pt\n",
      "[Train] Batch 50/917 loss=0.5453 bin_acc=0.977 type_acc=0.895\n",
      "[Train] Batch 100/917 loss=0.4512 bin_acc=0.994 type_acc=0.869\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, cfg.epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    107\u001b[39m     t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     tr = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     va = eval_epoch(model, loader_va, crit_bin, crit_type, cfg.device)\n\u001b[32m    110\u001b[39m     dt = time.time() - t0\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optim, crit_bin, crit_type, device, log_interval)\u001b[39m\n\u001b[32m     58\u001b[39m loss.backward()\n\u001b[32m     59\u001b[39m optim.step()\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * clips.size(\u001b[32m0\u001b[39m)\n\u001b[32m     62\u001b[39m running_loss += loss.item() * clips.size(\u001b[32m0\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# ðŸ‘‡ Print every N batches\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper to load any checkpoint\n",
    "def load_model(ckpt_path, device=\"cuda\"):\n",
    "    model = X3DMultiTask(num_classes=cfg.num_crime_classes).to(device)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    print(f\"[INFO] Loaded model from {ckpt_path}\")\n",
    "    return model\n",
    "\n",
    "# Load your three checkpoints\n",
    "model1 = load_model(\"x3d_multitask_epoch01.pt\")\n",
    "model2 = load_model(\"x3d_multitask_epoch02.pt\")\n",
    "model3 = load_model(\"x3d_multitask_best.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1b65f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\shrit/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded x3d_multitask_epoch01.pt\n",
      "[INFO] Extracted 19 clips from C:\\Users\\shrit\\Desktop\\Ml_Projects\\HackTheNest\\UCF101\\UCF-101\\ApplyLipstick\\v_ApplyLipstick_g01_c02.avi\n",
      "\n",
      "=== Crime Segments (epoch01) ===\n",
      "Clip 000: 0.0sâ€“4.3s | crime_prob=0.031 | type=Abuse (0.38)\n",
      "Clip 001: 0.3sâ€“4.5s | crime_prob=0.036 | type=Abuse (0.35)\n",
      "Clip 002: 0.5sâ€“4.8s | crime_prob=0.171 | type=Abuse (0.39)\n",
      "Clip 003: 0.8sâ€“5.1s | crime_prob=0.270 | type=Abuse (0.39)\n",
      "Clip 004: 1.1sâ€“5.3s | crime_prob=0.080 | type=Abuse (0.44)\n",
      "Clip 005: 1.3sâ€“5.6s | crime_prob=0.044 | type=Abuse (0.40)\n",
      "Clip 006: 1.6sâ€“5.9s | crime_prob=0.055 | type=Abuse (0.39)\n",
      "Clip 007: 1.9sâ€“6.1s | crime_prob=0.037 | type=Abuse (0.38)\n",
      "Clip 008: 2.1sâ€“6.4s | crime_prob=0.031 | type=Abuse (0.40)\n",
      "Clip 009: 2.4sâ€“6.7s | crime_prob=0.036 | type=Abuse (0.40)\n",
      "Clip 010: 2.7sâ€“6.9s | crime_prob=0.025 | type=Abuse (0.38)\n",
      "Clip 011: 2.9sâ€“7.2s | crime_prob=0.054 | type=Abuse (0.36)\n",
      "Clip 012: 3.2sâ€“7.5s | crime_prob=0.120 | type=Abuse (0.36)\n",
      "Clip 013: 3.5sâ€“7.7s | crime_prob=0.243 | type=Abuse (0.33)\n",
      "Clip 014: 3.7sâ€“8.0s | crime_prob=0.123 | type=Abuse (0.32)\n",
      "Clip 015: 4.0sâ€“8.3s | crime_prob=0.115 | type=Abuse (0.32)\n",
      "Clip 016: 4.3sâ€“8.5s | crime_prob=0.181 | type=Abuse (0.30)\n",
      "Clip 017: 4.5sâ€“8.8s | crime_prob=0.060 | type=Abuse (0.33)\n",
      "Clip 018: 4.8sâ€“9.1s | crime_prob=0.178 | type=Abuse (0.32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import decord\n",
    "\n",
    "# ========================\n",
    "# Class Names\n",
    "# ========================\n",
    "CLASS_NAMES = [\n",
    "    \"Abuse\", \"Arrest\", \"Arson\", \"Assault\", \"Burglary\",\n",
    "    \"Explosion\", \"Fighting\", \"RoadAccidents\", \"Robbery\",\n",
    "    \"Shooting\", \"Shoplifting\", \"Stealing\", \"Vandalism\"\n",
    "]\n",
    "\n",
    "# ========================\n",
    "# Model Definition\n",
    "# ========================\n",
    "class X3DMultiTask(nn.Module):\n",
    "    def __init__(self, backbone=None, num_classes=13):\n",
    "        super().__init__()\n",
    "        if backbone is None:\n",
    "            backbone = torch.hub.load(\"facebookresearch/pytorchvideo\", \"x3d_m\", pretrained=True)\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # locate classification head\n",
    "        if hasattr(self.backbone, \"head\") and hasattr(self.backbone.head, \"proj\"):\n",
    "            head = self.backbone.head\n",
    "        else:\n",
    "            head = self.backbone.blocks[-1]\n",
    "\n",
    "        feat_dim = head.proj.in_features\n",
    "        head.proj = nn.Identity()\n",
    "        if hasattr(head, \"activation\"):\n",
    "            head.activation = nn.Identity()\n",
    "\n",
    "        # new heads\n",
    "        self.head_bin = nn.Linear(feat_dim, 1)\n",
    "        self.head_type = nn.Linear(feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)  # (N, feat_dim)\n",
    "        s = self.head_bin(feats).squeeze(1)  # binary logits\n",
    "        c = self.head_type(feats)            # type logits\n",
    "        return s, c\n",
    "\n",
    "# ========================\n",
    "# Model Loader\n",
    "# ========================\n",
    "def load_model(weight_path, device=\"cuda\"):\n",
    "    model = X3DMultiTask(num_classes=len(CLASS_NAMES)).to(device)\n",
    "    state = torch.load(weight_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    print(f\"[INFO] Loaded {weight_path}\")\n",
    "    return model\n",
    "\n",
    "# ========================\n",
    "# Preprocess Video (matches training)\n",
    "# ========================\n",
    "def preprocess_video(video_path, clip_len=16, stride=8, frame_size=224, device=\"cuda\"):\n",
    "    vr = decord.VideoReader(video_path)\n",
    "    fnum = len(vr)\n",
    "\n",
    "    clips = []\n",
    "    for start in range(0, fnum - clip_len + 1, stride):\n",
    "        inds = list(range(start, start + clip_len))\n",
    "        inds = [min(i, fnum - 1) for i in inds]\n",
    "        frames = vr.get_batch(inds)\n",
    "        if hasattr(frames, \"asnumpy\"):\n",
    "            frames = frames.asnumpy()  # (T,H,W,3)\n",
    "        else:\n",
    "            frames = frames.cpu().numpy()\n",
    "\n",
    "        frames_resized = np.stack([\n",
    "            cv2.resize(fr, (frame_size, frame_size), interpolation=cv2.INTER_AREA)\n",
    "            for fr in frames\n",
    "        ], axis=0)\n",
    "\n",
    "        frames_resized = frames_resized.astype(np.float32) / 255.0\n",
    "        frames_t = torch.from_numpy(frames_resized).permute(3, 0, 1, 2)  # (3,T,H,W)\n",
    "\n",
    "        # Normalize\n",
    "        mean = torch.tensor([0.45, 0.45, 0.45]).view(3,1,1,1)\n",
    "        std  = torch.tensor([0.225, 0.225, 0.225]).view(3,1,1,1)\n",
    "        frames_t = (frames_t - mean) / std\n",
    "\n",
    "        clips.append(frames_t)\n",
    "\n",
    "    clips = torch.stack(clips, dim=0).to(device)  # (N,3,T,H,W)\n",
    "    print(f\"[INFO] Extracted {clips.shape[0]} clips from {video_path}\")\n",
    "    return clips\n",
    "\n",
    "# ========================\n",
    "# Inference\n",
    "# ========================\n",
    "def run_inference(model, clips, batch_size=16):\n",
    "    all_prob_bin, all_prob_type = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(clips), batch_size):\n",
    "            batch = clips[i:i+batch_size]\n",
    "            s, c = model(batch)\n",
    "            all_prob_bin.append(torch.sigmoid(s).cpu())\n",
    "            all_prob_type.append(torch.softmax(c, dim=1).cpu())\n",
    "    prob_bin = torch.cat(all_prob_bin, dim=0)\n",
    "    prob_type = torch.cat(all_prob_type, dim=0)\n",
    "    return prob_bin, prob_type\n",
    "\n",
    "# ========================\n",
    "# Segment Extraction\n",
    "# ========================\n",
    "def extract_segments(prob_bin, prob_type, clip_len, stride, fps=30, threshold=0.5):\n",
    "    prob_bin = prob_bin.numpy()\n",
    "    prob_type = prob_type.numpy()\n",
    "\n",
    "    segments = []\n",
    "    in_crime = False\n",
    "    start_idx = None\n",
    "\n",
    "    for i, p in enumerate(prob_bin):\n",
    "        if not in_crime and p >= threshold:\n",
    "            in_crime = True\n",
    "            start_idx = i\n",
    "        elif in_crime and p < threshold:\n",
    "            end_idx = i - 1\n",
    "            seg_types = prob_type[start_idx:end_idx+1].mean(0)\n",
    "            top_class = int(np.argmax(seg_types))\n",
    "            start_time = (start_idx * stride) / fps\n",
    "            end_time = ((end_idx + clip_len) * stride) / fps\n",
    "            segments.append((start_time, end_time, True, CLASS_NAMES[top_class]))\n",
    "            in_crime = False\n",
    "\n",
    "    if in_crime:\n",
    "        end_idx = len(prob_bin) - 1\n",
    "        seg_types = prob_type[start_idx:end_idx+1].mean(0)\n",
    "        top_class = int(np.argmax(seg_types))\n",
    "        start_time = (start_idx * stride) / fps\n",
    "        end_time = ((end_idx + clip_len) * stride) / fps\n",
    "        segments.append((start_time, end_time, True, CLASS_NAMES[top_class]))\n",
    "\n",
    "    return segments\n",
    "\n",
    "# ========================\n",
    "# Main\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\HackTheNest\\UCF101\\UCF-101\\ApplyLipstick\\v_ApplyLipstick_g01_c02.avi\"\n",
    "    model_path = \"x3d_multitask_epoch01.pt\"\n",
    "\n",
    "    model = load_model(model_path)\n",
    "    clips = preprocess_video(video_path)\n",
    "    prob_bin, prob_type = run_inference(model, clips)\n",
    "\n",
    "    segments = extract_segments(prob_bin, prob_type, clip_len=16, stride=8, fps=30, threshold=9.7)\n",
    "\n",
    "    print(\"\\n=== Crime Segments (epoch01) ===\")\n",
    "    for i, p in enumerate(prob_bin):\n",
    "        start_time = (i * 8) / 30       # stride=8, fps=30\n",
    "        end_time   = ((i + 16) * 8) / 30\n",
    "        top_class = prob_type[i].argmax().item()\n",
    "        top_score = prob_type[i].max().item()\n",
    "        print(f\"Clip {i:03d}: {start_time:.1f}sâ€“{end_time:.1f}s | \"\n",
    "            f\"crime_prob={p.item():.3f} | \"\n",
    "            f\"type={CLASS_NAMES[top_class]} ({top_score:.2f})\")\n",
    "    for seg in segments:\n",
    "        start, end, is_crime, crime_type = seg\n",
    "        print(f\"Start={start:.1f}s, End={end:.1f}s, Crime={is_crime}, Type={crime_type}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf8264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caughtIN4k",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
